{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o84CpzVAPmwH",
        "outputId": "171cd82c-18f5-49ae-cf32-70ec9a38c66c"
      },
      "outputs": [
       
      ],
      "source": [
        "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
        "!pip install imagecodecs-lite\n",
        "!pip install imagecodecs\n",
        "!pip uninstall tiffile\n",
        "import imagecodecs\n",
        "from skimage.io import imread, imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from random import shuffle\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def load_data(path):\n",
        "    info = []\n",
        "    images_list = []\n",
        "    images_class = []\n",
        "    print('Loading ', path, 'in memory...')\n",
        "    with open(path, newline='') as csv_file:\n",
        "        images_reader = csv.reader(csv_file)\n",
        "        next(images_reader)\n",
        "        for row in images_reader:\n",
        "            info.append(row)\n",
        "    shuffle(info)\n",
        "\n",
        "    for elem in info:\n",
        "        images_list.append(imread('/content/drive/MyDrive/Colab Notebooks/Train&Validation/' + elem[0])[55:455, 55:455].reshape(400, 400, 1))\n",
        "        images_class.append(elem[1] == 'covid')\n",
        "    return np.array(images_list), np.array(images_class)\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\", input_shape=(400, 400, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "    model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "    model.add(Conv2D(128, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "    model.add(Dropout(0.25))\n",
        "    \n",
        "\n",
        "    model.add(Conv2D(256, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Conv2D(128, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "    model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPool2D())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation=\"relu\"))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 60,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "\n",
        "Adam = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        "def run_model2(path, epochs_number, folder): #Save acc, loss,... in a file to allow interruption\n",
        "    try:\n",
        "        my_model = load_model('/content/drive/MyDrive/Colab Notebooks/' + path)\n",
        "        print('\\033[94m'+'\\033[1m' + 'Model Loaded' + '\\033[0m')\n",
        "        my_model.summary()\n",
        "    except OSError:\n",
        "        print('\\033[91m'+'\\033[1m'+'Model', path, 'not found. Creating a new one'+'\\033[0m')\n",
        "        my_model = create_model()\n",
        "        my_model.summary()\n",
        "        my_model.compile(loss=tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size'),\n",
        "                         optimizer=Adam, metrics=[\"accuracy\"])\n",
        "\n",
        "    train_images, train_classes = load_data('/content/drive/MyDrive/Colab Notebooks/CSV/train' + str(folder) + '.csv')\n",
        "    validation_images, validation_classes = load_data('/content/drive/MyDrive/Colab Notebooks/CSV/validation' + str(folder) + '.csv')\n",
        "    datagen.fit(train_images)\n",
        "    history = my_model.fit(train_images, train_classes, epochs=epochs_number, validation_data=(validation_images,\n",
        "                                                                                               validation_classes))\n",
        "    my_model.save('/content/drive/MyDrive/Colab Notebooks/' + path)\n",
        "    return history.history['accuracy'], history.history['val_accuracy'], history.history['loss'], history.history['val_loss'], epochs_number\n",
        "\n",
        "\n",
        "def plot_accuracy(accuracy, validation_accuracy, loss, validation_loss, epochs): #Add save as png\n",
        "    epochs_range = range(epochs)\n",
        "\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs_range, accuracy, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "accuracy, validation_accuracy, loss, validation_loss, epochs = run_model2('23-06-22-1.h5', 200, 1)\n",
        "\n",
        "plot_accuracy(accuracy, validation_accuracy, loss, validation_loss, epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CT.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
